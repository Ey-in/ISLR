{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDKBIIkgqAFL",
        "outputId": "bf185e42-bf5c-49ea-f21f-0daf4949a6d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: './drive/MyDrive'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd ./drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IpEQKDrDqAFP",
        "outputId": "81c64e3b-67d7-4eb1-df0c-e65fef40aef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.2)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.23.5)\n",
            "Requirement already satisfied: tf_utils in /usr/local/lib/python3.11/dist-packages (0.0.1)\n",
            "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from tf_utils) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (4.25.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (4.13.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.0.0->tf_utils) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0.0->tf_utils) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow>=2.0.0->tf_utils) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow>=2.0.0->tf_utils) (0.5.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow>=2.0.0->tf_utils) (1.15.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.0.0->tf_utils) (3.2.2)\n",
            "Collecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\n",
            "orbax-checkpoint 0.11.12 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "6902fb71f05e4e5db58b9e435fb3e9f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jax==0.5.1\n",
            "  Using cached jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.1,>=0.5.1 (from jax==0.5.1)\n",
            "  Downloading jaxlib-0.5.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax==0.5.1) (0.5.1)\n",
            "Collecting numpy>=1.25 (from jax==0.5.1)\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax==0.5.1) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax==0.5.1) (1.15.2)\n",
            "Downloading jax-0.5.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.5.1-cp311-cp311-manylinux2014_x86_64.whl (105.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, jaxlib, jax\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.3\n",
            "    Uninstalling numpy-1.24.3:\n",
            "      Successfully uninstalled numpy-1.24.3\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.30\n",
            "    Uninstalling jaxlib-0.4.30:\n",
            "      Successfully uninstalled jaxlib-0.4.30\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.30\n",
            "    Uninstalling jax-0.4.30:\n",
            "      Successfully uninstalled jax-0.4.30\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.5.1 jaxlib-0.5.1 numpy-2.2.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "jax",
                  "jaxlib"
                ]
              },
              "id": "b859a23106604b349b4bb2d45ce5dc9d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -q tensorflow==2.12.0\n",
        "!pip install -q tensorflow-addons==0.20.0\n",
        "!pip install -q git+https://github.com/hoyso48/tf-utils@main\n",
        "!pip install --upgrade scipy\n",
        "!pip install tf_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "wD7tqFC_qAFQ",
        "outputId": "ffec7016-2970-4270-e122-c5aeb0081a19"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras._tf_keras'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f8b6455d81d0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Local project imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/activations/gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m ]\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mInitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInitializer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mRegularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegularizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mConstraint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConstraint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Update this object's dict so that if someone keeps a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m#   LazyLoader, lookups are efficient (__getattr__ is only called on lookups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;31m#   that fail).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras._tf_keras'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import tensorflow.keras.mixed_precision as mixed_precision\n",
        "\n",
        "from tqdm.autonotebook import tqdm\n",
        "import sklearn\n",
        "\n",
        "from tf_utils.schedules import OneCycleLR, ListedLR\n",
        "from tf_utils.callbacks import Snapshot, SWA\n",
        "from tf_utils.learners import FGM, AWP\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import math\n",
        "import random\n",
        "import sys\n",
        "import cv2\n",
        "import gc\n",
        "import glob\n",
        "import datetime\n",
        "\n",
        "print(f'Tensorflow Version: {tf.__version__}')\n",
        "print(f'Python Version: {sys.version}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u74o98JxqAFQ"
      },
      "outputs": [],
      "source": [
        "# Seed all random number generators\n",
        "def seed_everything(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "def get_strategy(device='TPU'):\n",
        "    if \"TPU\" in device:\n",
        "        tpu = 'local' if device=='TPU-VM' else None\n",
        "        print(\"connecting to TPU...\")\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=tpu)\n",
        "        strategy = tf.distribute.TPUStrategy(tpu)\n",
        "        IS_TPU = True\n",
        "\n",
        "    if device == \"GPU\"  or device==\"CPU\":\n",
        "        ngpu = len(tf.config.experimental.list_physical_devices('GPU'))\n",
        "        if ngpu>1:\n",
        "            print(\"Using multi GPU\")\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "        elif ngpu==1:\n",
        "            print(\"Using single GPU\")\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "        else:\n",
        "            print(\"Using CPU\")\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "            CFG.device = \"CPU\"\n",
        "\n",
        "    if device == \"GPU\":\n",
        "        print(\"Num GPUs Available: \", ngpu)\n",
        "\n",
        "    AUTO     = tf.data.experimental.AUTOTUNE\n",
        "    REPLICAS = strategy.num_replicas_in_sync\n",
        "    print(f'REPLICAS: {REPLICAS}')\n",
        "\n",
        "    return strategy, REPLICAS, IS_TPU\n",
        "\n",
        "STRATEGY, N_REPLICAS, IS_TPU = get_strategy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVDc3vkwqAFQ"
      },
      "outputs": [],
      "source": [
        "#NOTE: you should run KaggleDatasets.get_gcs_path(dataset_name) in the kaggle notebook to update gcs_path as they expires after several weeks..\n",
        "#notebook: https://www.kaggle.com/code/hoyso48/islr-get-gcs-path\n",
        "\n",
        "GCS_PATH = {\n",
        "            'ISLR':'gs://kds-bf6468ebf8d3f83cb78ec34aab656e5f2249470042dabba5a5b022c4',\n",
        "            '5fold':'gs://kds-3d2c4f3f70f22e7b8793b9d05e1e012047f4600d7ca765ccca45aa52',\n",
        "            '5fold_randsplit':'gs://kds-cfd559924f5f32aae240670f7625264cb57d4c0152af5703f5e13a99',\n",
        "            }\n",
        "\n",
        "TRAIN_FILENAMES = tf.io.gfile.glob(GCS_PATH['5fold']+'/*.tfrecords')\n",
        "COMPETITION_PATH = GCS_PATH['ISLR']\n",
        "\n",
        "print(len(TRAIN_FILENAMES))\n",
        "!gsutil cp {COMPETITION_PATH}/train.csv .\n",
        "!gsutil cp {COMPETITION_PATH}/sign_to_prediction_index_map.json ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtrBI-jwqAFQ"
      },
      "outputs": [],
      "source": [
        "# Train DataFrame\n",
        "train_df = pd.read_csv('train.csv')\n",
        "display(train_df.head())\n",
        "display(train_df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAc9FKztqAFQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def count_data_items(filenames):\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename.split('/')[-1]).group(1)) for filename in filenames]\n",
        "    return np.sum(n)\n",
        "print(count_data_items(TRAIN_FILENAMES), len(train_df))\n",
        "assert count_data_items(TRAIN_FILENAMES) == len(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xyloTyiqAFQ"
      },
      "outputs": [],
      "source": [
        "ROWS_PER_FRAME = 543\n",
        "MAX_LEN = 384\n",
        "CROP_LEN = MAX_LEN\n",
        "NUM_CLASSES  = 250\n",
        "PAD = -100.\n",
        "NOSE=[\n",
        "    1,2,98,327\n",
        "]\n",
        "LNOSE = [98]\n",
        "RNOSE = [327]\n",
        "LIP = [ 0,\n",
        "    61, 185, 40, 39, 37, 267, 269, 270, 409,\n",
        "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
        "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
        "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
        "]\n",
        "LLIP = [84,181,91,146,61,185,40,39,37,87,178,88,95,78,191,80,81,82]\n",
        "RLIP = [314,405,321,375,291,409,270,269,267,317,402,318,324,308,415,310,311,312]\n",
        "\n",
        "POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n",
        "LPOSE = [513,505,503,501]\n",
        "RPOSE = [512,504,502,500]\n",
        "\n",
        "REYE = [\n",
        "    33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
        "    246, 161, 160, 159, 158, 157, 173,\n",
        "]\n",
        "LEYE = [\n",
        "    263, 249, 390, 373, 374, 380, 381, 382, 362,\n",
        "    466, 388, 387, 386, 385, 384, 398,\n",
        "]\n",
        "\n",
        "LHAND = np.arange(468, 489).tolist()\n",
        "RHAND = np.arange(522, 543).tolist()\n",
        "\n",
        "POINT_LANDMARKS = LIP + LHAND + RHAND + NOSE + REYE + LEYE #+POSE\n",
        "\n",
        "NUM_NODES = len(POINT_LANDMARKS)\n",
        "CHANNELS = 6*NUM_NODES\n",
        "\n",
        "print(NUM_NODES)\n",
        "print(CHANNELS)\n",
        "\n",
        "def interp1d_(x, target_len, method='random'):\n",
        "    length = tf.shape(x)[1]\n",
        "    target_len = tf.maximum(1,target_len)\n",
        "    if method == 'random':\n",
        "        if tf.random.uniform(()) < 0.33:\n",
        "            x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bilinear')\n",
        "        else:\n",
        "            if tf.random.uniform(()) < 0.5:\n",
        "                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bicubic')\n",
        "            else:\n",
        "                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'nearest')\n",
        "    else:\n",
        "        x = tf.image.resize(x, (target_len,tf.shape(x)[1]),method)\n",
        "    return x\n",
        "\n",
        "def tf_nan_mean(x, axis=0, keepdims=False):\n",
        "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n",
        "\n",
        "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
        "    if center is None:\n",
        "        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n",
        "    d = x - center\n",
        "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
        "\n",
        "class Preprocess(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_len=MAX_LEN, point_landmarks=POINT_LANDMARKS, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.max_len = max_len\n",
        "        self.point_landmarks = point_landmarks\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if tf.rank(inputs) == 3:\n",
        "            x = inputs[None,...]\n",
        "        else:\n",
        "            x = inputs\n",
        "\n",
        "        mean = tf_nan_mean(tf.gather(x, [17], axis=2), axis=[1,2], keepdims=True)\n",
        "        mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5,x.dtype), mean)\n",
        "        x = tf.gather(x, self.point_landmarks, axis=2) #N,T,P,C\n",
        "        std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n",
        "\n",
        "        x = (x - mean)/std\n",
        "\n",
        "        if self.max_len is not None:\n",
        "            x = x[:,:self.max_len]\n",
        "        length = tf.shape(x)[1]\n",
        "        x = x[...,:2]\n",
        "\n",
        "        dx = tf.cond(tf.shape(x)[1]>1,lambda:tf.pad(x[:,1:] - x[:,:-1], [[0,0],[0,1],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
        "\n",
        "        dx2 = tf.cond(tf.shape(x)[1]>2,lambda:tf.pad(x[:,2:] - x[:,:-2], [[0,0],[0,2],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
        "\n",
        "        x = tf.concat([\n",
        "            tf.reshape(x, (-1,length,2*len(self.point_landmarks))),\n",
        "            tf.reshape(dx, (-1,length,2*len(self.point_landmarks))),\n",
        "            tf.reshape(dx2, (-1,length,2*len(self.point_landmarks))),\n",
        "        ], axis = -1)\n",
        "\n",
        "        x = tf.where(tf.math.is_nan(x),tf.constant(0.,x.dtype),x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r17ZnZaGqAFQ"
      },
      "outputs": [],
      "source": [
        "def decode_tfrec(record_bytes):\n",
        "    features = tf.io.parse_single_example(record_bytes, {\n",
        "        'coordinates': tf.io.FixedLenFeature([], tf.string),\n",
        "        'sign': tf.io.FixedLenFeature([], tf.int64),\n",
        "    })\n",
        "    out = {}\n",
        "    out['coordinates']  = tf.reshape(tf.io.decode_raw(features['coordinates'], tf.float32), (-1,ROWS_PER_FRAME,3))\n",
        "    out['sign'] = features['sign']\n",
        "    return out\n",
        "\n",
        "def filter_nans_tf(x, ref_point=POINT_LANDMARKS):\n",
        "    mask = tf.math.logical_not(tf.reduce_all(tf.math.is_nan(tf.gather(x,ref_point,axis=1)), axis=[-2,-1]))\n",
        "    x = tf.boolean_mask(x, mask, axis=0)\n",
        "    return x\n",
        "\n",
        "def preprocess(x, augment=False, max_len=MAX_LEN):\n",
        "    coord = x['coordinates']\n",
        "    coord = filter_nans_tf(coord)\n",
        "    if augment:\n",
        "        coord = augment_fn(coord, max_len=max_len)\n",
        "    coord = tf.ensure_shape(coord, (None,ROWS_PER_FRAME,3))\n",
        "\n",
        "    return tf.cast(Preprocess(max_len=max_len)(coord)[0],tf.float32), tf.one_hot(x['sign'], NUM_CLASSES)\n",
        "\n",
        "def flip_lr(x):\n",
        "    x,y,z = tf.unstack(x, axis=-1)\n",
        "    x = 1-x\n",
        "    new_x = tf.stack([x,y,z], -1)\n",
        "    new_x = tf.transpose(new_x, [1,0,2])\n",
        "    lhand = tf.gather(new_x, LHAND, axis=0)\n",
        "    rhand = tf.gather(new_x, RHAND, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LHAND)[...,None], rhand)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RHAND)[...,None], lhand)\n",
        "    llip = tf.gather(new_x, LLIP, axis=0)\n",
        "    rlip = tf.gather(new_x, RLIP, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LLIP)[...,None], rlip)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RLIP)[...,None], llip)\n",
        "    lpose = tf.gather(new_x, LPOSE, axis=0)\n",
        "    rpose = tf.gather(new_x, RPOSE, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LPOSE)[...,None], rpose)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RPOSE)[...,None], lpose)\n",
        "    leye = tf.gather(new_x, LEYE, axis=0)\n",
        "    reye = tf.gather(new_x, REYE, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LEYE)[...,None], reye)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(REYE)[...,None], leye)\n",
        "    lnose = tf.gather(new_x, LNOSE, axis=0)\n",
        "    rnose = tf.gather(new_x, RNOSE, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LNOSE)[...,None], rnose)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RNOSE)[...,None], lnose)\n",
        "    new_x = tf.transpose(new_x, [1,0,2])\n",
        "    return new_x\n",
        "\n",
        "def resample(x, rate=(0.8,1.2)):\n",
        "    rate = tf.random.uniform((), rate[0], rate[1])\n",
        "    length = tf.shape(x)[0]\n",
        "    new_size = tf.cast(rate*tf.cast(length,tf.float32), tf.int32) #(tf.cast(rate*tf.cast(length,tf.float32), tf.int32),tf.shape(x)[1])\n",
        "    new_x = interp1d_(x, new_size) #tf.image.resize(x, new_size, method='bilinear')\n",
        "    return new_x\n",
        "\n",
        "def spatial_random_affine(xyz,\n",
        "    scale  = (0.8,1.2),\n",
        "    shear = (-0.15,0.15),\n",
        "    shift  = (-0.1,0.1),\n",
        "    degree = (-30,30),\n",
        "):\n",
        "    center = tf.constant([0.5,0.5])\n",
        "    if scale is not None:\n",
        "        scale = tf.random.uniform((),*scale)\n",
        "        xyz = scale*xyz\n",
        "\n",
        "    if shear is not None:\n",
        "        xy = xyz[...,:2]\n",
        "        z = xyz[...,2:]\n",
        "        shear_x = shear_y = tf.random.uniform((),*shear)\n",
        "        if tf.random.uniform(()) < 0.5:\n",
        "            shear_x = 0.\n",
        "        else:\n",
        "            shear_y = 0.\n",
        "        shear_mat = tf.identity([\n",
        "            [1.,shear_x],\n",
        "            [shear_y,1.]\n",
        "        ])\n",
        "        xy = xy @ shear_mat\n",
        "        center = center + [shear_y, shear_x]\n",
        "        xyz = tf.concat([xy,z], axis=-1)\n",
        "\n",
        "    if degree is not None:\n",
        "        xy = xyz[...,:2]\n",
        "        z = xyz[...,2:]\n",
        "        xy -= center\n",
        "        degree = tf.random.uniform((),*degree)\n",
        "        radian = degree/180*np.pi\n",
        "        c = tf.math.cos(radian)\n",
        "        s = tf.math.sin(radian)\n",
        "        rotate_mat = tf.identity([\n",
        "            [c,s],\n",
        "            [-s, c],\n",
        "        ])\n",
        "        xy = xy @ rotate_mat\n",
        "        xy = xy + center\n",
        "        xyz = tf.concat([xy,z], axis=-1)\n",
        "\n",
        "    if shift is not None:\n",
        "        shift = tf.random.uniform((),*shift)\n",
        "        xyz = xyz + shift\n",
        "\n",
        "    return xyz\n",
        "\n",
        "def temporal_crop(x, length=MAX_LEN):\n",
        "    l = tf.shape(x)[0]\n",
        "    offset = tf.random.uniform((), 0, tf.clip_by_value(l-length,1,length), dtype=tf.int32)\n",
        "    x = x[offset:offset+length]\n",
        "    return x\n",
        "\n",
        "def temporal_mask(x, size=(0.2,0.4), mask_value=float('nan')):\n",
        "    l = tf.shape(x)[0]\n",
        "    mask_size = tf.random.uniform((), *size)\n",
        "    mask_size = tf.cast(tf.cast(l, tf.float32) * mask_size, tf.int32)\n",
        "    mask_offset = tf.random.uniform((), 0, tf.clip_by_value(l-mask_size,1,l), dtype=tf.int32)\n",
        "    x = tf.tensor_scatter_nd_update(x,tf.range(mask_offset, mask_offset+mask_size)[...,None],tf.fill([mask_size,543,3],mask_value))\n",
        "    return x\n",
        "\n",
        "def spatial_mask(x, size=(0.2,0.4), mask_value=float('nan')):\n",
        "    mask_offset_y = tf.random.uniform(())\n",
        "    mask_offset_x = tf.random.uniform(())\n",
        "    mask_size = tf.random.uniform((), *size)\n",
        "    mask_x = (mask_offset_x<x[...,0]) & (x[...,0] < mask_offset_x + mask_size)\n",
        "    mask_y = (mask_offset_y<x[...,1]) & (x[...,1] < mask_offset_y + mask_size)\n",
        "    mask = mask_x & mask_y\n",
        "    x = tf.where(mask[...,None], mask_value, x)\n",
        "    return x\n",
        "\n",
        "def augment_fn(x, always=False, max_len=None):\n",
        "    if tf.random.uniform(())<0.8 or always:\n",
        "        x = resample(x, (0.5,1.5))\n",
        "    if tf.random.uniform(())<0.5 or always:\n",
        "        x = flip_lr(x)\n",
        "    if max_len is not None:\n",
        "        x = temporal_crop(x, max_len)\n",
        "    if tf.random.uniform(())<0.75 or always:\n",
        "        x = spatial_random_affine(x)\n",
        "    if tf.random.uniform(())<0.5 or always:\n",
        "        x = temporal_mask(x)\n",
        "    if tf.random.uniform(())<0.5 or always:\n",
        "        x = spatial_mask(x)\n",
        "    return x\n",
        "\n",
        "def get_tfrec_dataset(tfrecords, batch_size=64, max_len=64, drop_remainder=False, augment=False, shuffle=False, repeat=False):\n",
        "    # Initialize dataset with TFRecords\n",
        "    ds = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=tf.data.AUTOTUNE, compression_type='GZIP')\n",
        "    ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n",
        "    ds = ds.map(lambda x: preprocess(x, augment=augment, max_len=max_len), tf.data.AUTOTUNE)\n",
        "\n",
        "    if repeat:\n",
        "        ds = ds.repeat()\n",
        "\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(shuffle)\n",
        "        options = tf.data.Options()\n",
        "        options.experimental_deterministic = (False)\n",
        "        ds = ds.with_options(options)\n",
        "\n",
        "    if batch_size:\n",
        "        ds = ds.padded_batch(batch_size, padding_values=PAD, padded_shapes=([max_len,CHANNELS],[NUM_CLASSES]), drop_remainder=drop_remainder)\n",
        "\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def efficient_cutmix(samples, labels, alpha=0.7, cutmix_ratio=0.2):\n",
        "    \"\"\"\n",
        "    高效的CutMix实现，使用向量化操作代替循环\n",
        "    适用于手语识别的时序数据\n",
        "\n",
        "    参数:\n",
        "        samples: 输入特征 [batch_size, time_steps, channels]\n",
        "        labels: 标签 [batch_size, num_classes]\n",
        "        alpha: 混合权重\n",
        "        cutmix_ratio: 要处理的批次中样本的比例\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(samples)[0]\n",
        "\n",
        "    # 仅在批次大小足够时执行CutMix\n",
        "    def do_mix():\n",
        "        # 创建随机排列的索引\n",
        "        perm = tf.random.shuffle(tf.range(batch_size))\n",
        "\n",
        "        # 创建CutMix掩码 - 决定哪些样本会被混合\n",
        "        cutmix_samples = tf.cast(tf.random.uniform([batch_size]) < cutmix_ratio, tf.float32)\n",
        "        cutmix_samples = tf.reshape(cutmix_samples, [-1, 1, 1])  # 广播到时间和特征维度\n",
        "\n",
        "        # 创建手部和嘴唇区域的掩码\n",
        "        # 预先计算这些掩码可以避免每个样本都执行复杂的索引操作\n",
        "        channels = tf.shape(samples)[2]\n",
        "        feature_mask = tf.zeros([1, 1, channels], dtype=tf.float32)\n",
        "\n",
        "        # 构建简化的特征掩码 - 我们将样本分为三部分：\n",
        "        # 1. 需要重点混合的区域 (掩码值 = 1.0)\n",
        "        # 2. 轻度混合的区域 (掩码值 = 0.5)\n",
        "        # 3. 保持原样的区域 (掩码值 = 0.0)\n",
        "\n",
        "        # 随机生成一个三区域掩码，而不是精确针对手部/嘴唇\n",
        "        # 这样可以大大提高效率，同时仍然提供多样化的混合\n",
        "        random_mask = tf.random.uniform([1, 1, channels], 0, 1)\n",
        "\n",
        "        # 创建三区域掩码\n",
        "        high_mix_mask = tf.cast(random_mask < 0.3, tf.float32)  # 30% 通道重点混合\n",
        "        medium_mix_mask = tf.cast((random_mask >= 0.3) & (random_mask < 0.6), tf.float32)  # 30% 通道轻度混合\n",
        "\n",
        "        # 混合样本和置换样本\n",
        "        mixed_samples = (\n",
        "            (1 - cutmix_samples * high_mix_mask * 0.8) * samples +  # 原始样本，高混合区域减少80%\n",
        "            (cutmix_samples * high_mix_mask * 0.8) * tf.gather(samples, perm) +  # 置换样本，高混合区域占80%\n",
        "\n",
        "            (1 - cutmix_samples * medium_mix_mask * 0.4) * samples +  # 原始样本，中混合区域减少40%\n",
        "            (cutmix_samples * medium_mix_mask * 0.4) * tf.gather(samples, perm)  # 置换样本，中混合区域占40%\n",
        "        )\n",
        "\n",
        "        # 混合标签 - 使用简单的线性插值\n",
        "        cutmix_labels = tf.reshape(cutmix_samples, [-1, 1])  # 广播到标签维度\n",
        "        mixed_labels = (\n",
        "            (1 - cutmix_labels * alpha) * labels +\n",
        "            (cutmix_labels * alpha) * tf.gather(labels, perm)\n",
        "        )\n",
        "\n",
        "        return mixed_samples, mixed_labels\n",
        "\n",
        "    def no_mix():\n",
        "        return samples, labels\n",
        "\n",
        "    # 只有当批次大小 > 1 时才执行CutMix\n",
        "    return tf.cond(batch_size > 1, do_mix, no_mix)"
      ],
      "metadata": {
        "id": "cJwalyeMuZYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbVSTH9xqAFQ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "def filter_nans(frames):\n",
        "    return frames[~np.isnan(frames).all(axis=(-2,-1))]\n",
        "\n",
        "ds = tf.data.TFRecordDataset(TRAIN_FILENAMES, num_parallel_reads=tf.data.AUTOTUNE, compression_type='GZIP')\n",
        "ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n",
        "print(ds)\n",
        "for x in ds:\n",
        "    temp = x['coordinates'].numpy()\n",
        "    if not len(filter_nans(temp[:,LHAND])) == 0:\n",
        "        break\n",
        "\n",
        "#调用数据增强\n",
        "ds = get_tfrec_dataset(TRAIN_FILENAMES, augment=True, batch_size=1024)\n",
        "# 打印数据集信息\n",
        "for samples, labels in ds.take(1):\n",
        "    print(\"Dataset samples shape:\", samples.shape)\n",
        "    print(\"Dataset labels shape:\", labels.shape)\n",
        "\n",
        "ds_cutmix = augment_dataset_with_cutmix(ds, cutmix_prob=0.5, cutmix_ratio=0.2)\n",
        "\n",
        "\n",
        "edges = [(0,1),(1,2),(2,3),(3,4),(0,5),(0,17),(5,6),(6,7),(7,8),(5,9),(9,10),(10,11),(11,12),\n",
        "         (9,13),(13,14),(14,15),(15,16),(13,17),(17,18),(18,19),(19,20)]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "def plot_frame(frame, edges=[], idxs=[]):\n",
        "\n",
        "    frame[np.isnan(frame)] = 0\n",
        "    x = list(frame[...,0])\n",
        "    y = list(frame[...,1])\n",
        "    if len(idxs) == 0:\n",
        "        idxs = list(range(len(x)))\n",
        "    ax.clear()\n",
        "    ax.scatter(x, y, color='dodgerblue')\n",
        "    for i in range(len(x)):\n",
        "        ax.text(x[i], y[i], idxs[i])\n",
        "\n",
        "    for edge in edges:\n",
        "        ax.plot([x[edge[0]], x[edge[1]]], [y[edge[0]], y[edge[1]]], color='salmon')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "\n",
        "def animate_frames(frames, edges=[], idxs=[]):\n",
        "    anim = FuncAnimation(fig, lambda frame: plot_frame(frame, edges, idxs), frames=frames, interval=100)\n",
        "    return HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaIHc89AqAFQ"
      },
      "outputs": [],
      "source": [
        "class ECA(tf.keras.layers.Layer):\n",
        "    def __init__(self, kernel_size=5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
        "        nn = tf.expand_dims(nn, -1)\n",
        "        nn = self.conv(nn)\n",
        "        nn = tf.squeeze(nn, -1)\n",
        "        nn = tf.nn.sigmoid(nn)\n",
        "        nn = nn[:,None,:]\n",
        "        return inputs * nn\n",
        "\n",
        "class LateDropout(tf.keras.layers.Layer):\n",
        "    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.rate = rate\n",
        "        self.start_step = start_step\n",
        "        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
        "        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = tf.cond(self._train_counter < self.start_step, lambda:inputs, lambda:self.dropout(inputs, training=training))\n",
        "        if training:\n",
        "          self._train_counter.assign_add(1)\n",
        "        return x\n",
        "\n",
        "class CausalDWConv1D(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "        kernel_size=17,\n",
        "        dilation_rate=1,\n",
        "        use_bias=False,\n",
        "        depthwise_initializer='glorot_uniform',\n",
        "        name='', **kwargs):\n",
        "        super().__init__(name=name,**kwargs)\n",
        "        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
        "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
        "                            kernel_size,\n",
        "                            strides=1,\n",
        "                            dilation_rate=dilation_rate,\n",
        "                            padding='valid',\n",
        "                            use_bias=use_bias,\n",
        "                            depthwise_initializer=depthwise_initializer,\n",
        "                            name=name + '_dwconv')\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.causal_pad(inputs)\n",
        "        x = self.dw_conv(x)\n",
        "        return x\n",
        "\n",
        "def Conv1DBlock(channel_size,\n",
        "          kernel_size,\n",
        "          dilation_rate=1,\n",
        "          drop_rate=0.0,\n",
        "          expand_ratio=2,\n",
        "          se_ratio=0.25,\n",
        "          activation='swish',\n",
        "          name=None):\n",
        "    '''\n",
        "    efficient conv1d block, @hoyso48\n",
        "    '''\n",
        "    if name is None:\n",
        "        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
        "    # Expansion phase\n",
        "    def apply(inputs):\n",
        "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
        "        channels_expand = channels_in * expand_ratio\n",
        "\n",
        "        skip = inputs\n",
        "\n",
        "        x = tf.keras.layers.Dense(\n",
        "            channels_expand,\n",
        "            use_bias=True,\n",
        "            activation=activation,\n",
        "            name=name + '_expand_conv')(inputs)\n",
        "\n",
        "        # Depthwise Convolution\n",
        "        x = CausalDWConv1D(kernel_size,\n",
        "            dilation_rate=dilation_rate,\n",
        "            use_bias=False,\n",
        "            name=name + '_dwconv')(x)\n",
        "\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn')(x)\n",
        "\n",
        "        x  = ECA()(x)\n",
        "\n",
        "        x = tf.keras.layers.Dense(\n",
        "            channel_size,\n",
        "            use_bias=True,\n",
        "            name=name + '_project_conv')(x)\n",
        "\n",
        "        if drop_rate > 0:\n",
        "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n",
        "\n",
        "        if (channels_in == channel_size):\n",
        "            x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
        "        return x\n",
        "\n",
        "    return apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hPmJX0YqAFR"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.scale = self.dim ** -0.5\n",
        "        self.num_heads = num_heads\n",
        "        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
        "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        qkv = self.qkv(inputs)\n",
        "        qkv = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n",
        "        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
        "\n",
        "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, None, :]\n",
        "\n",
        "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
        "        attn = self.drop1(attn)\n",
        "\n",
        "        x = attn @ v\n",
        "        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "def TransformerBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish'):\n",
        "    def apply(inputs):\n",
        "        x = inputs\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
        "        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout)(x)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
        "        x = tf.keras.layers.Add()([inputs, x])\n",
        "        attn_out = x\n",
        "\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
        "        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation)(x)\n",
        "        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
        "        x = tf.keras.layers.Add()([attn_out, x])\n",
        "        return x\n",
        "    return apply"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 新增\n",
        "class WindowedAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim=256, num_heads=4, window_size=16, shift=False, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift = shift\n",
        "\n",
        "        # 相对位置编码矩阵 (2*window_size-1)\n",
        "        self.rel_pos_bias = self.add_weight(\n",
        "            name=\"rel_pos_bias\",\n",
        "            shape=(2 * window_size - 1, num_heads),\n",
        "            initializer=\"zeros\",\n",
        "        )\n",
        "\n",
        "        self.qkv = tf.keras.layers.Dense(3*dim)\n",
        "        self.proj = tf.keras.layers.Dense(dim)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def get_relative_positions(self, length):\n",
        "        # 生成相对位置索引矩阵\n",
        "        range_vec = tf.range(length)\n",
        "        distance_mat = range_vec[:, None] - range_vec[None, :]\n",
        "        return distance_mat + self.window_size - 1  # 偏移至非负数\n",
        "\n",
        "    def window_partition(self, x):\n",
        "        # x: (B, T, C)\n",
        "        B, T, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
        "\n",
        "        # 移位操作\n",
        "        if self.shift:\n",
        "            x = tf.roll(x, shift=-self.window_size//2, axis=1)\n",
        "\n",
        "        # 填充与分窗\n",
        "        pad_len = (self.window_size - T % self.window_size) % self.window_size\n",
        "        x = tf.pad(x, [[0,0], [0,pad_len], [0,0]])\n",
        "        x = tf.reshape(x, [B, -1, self.window_size, C])  # (B, num_win, win_size, C)\n",
        "        return x, pad_len\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        B, T, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
        "\n",
        "        # 分窗处理\n",
        "        x_windows, pad_len = self.window_partition(x)  # (B, nW, win, C)\n",
        "        x_windows = tf.reshape(x_windows, [-1, self.window_size, C])  # (B*nW, win, C)\n",
        "\n",
        "        # 生成QKV\n",
        "        qkv = self.qkv(x_windows)\n",
        "        qkv = tf.reshape(qkv, [-1, self.window_size, 3, self.num_heads, C//self.num_heads])\n",
        "        q, k, v = tf.unstack(qkv, axis=2)  # 各 (B*nW, win, nH, C/nH)\n",
        "\n",
        "        # 相对位置编码\n",
        "        rel_pos = self.get_relative_positions(self.window_size)\n",
        "        rel_bias = tf.gather(self.rel_pos_bias, rel_pos)  # (win, win, nH)\n",
        "        rel_bias = tf.transpose(rel_bias, [2,0,1])  # (nH, win, win)\n",
        "\n",
        "        # 注意力计算\n",
        "        attn = tf.einsum('bqhd,bkhd->bhqk', q, k)  # (B*nW, nH, win, win)\n",
        "        attn = attn + rel_bias[None,...]  # 加入位置偏置\n",
        "        attn = attn / tf.sqrt(tf.cast(C//self.num_heads, tf.float32))\n",
        "\n",
        "        if mask is not None:\n",
        "            # 继承原有mask机制\n",
        "            mask = tf.reshape(mask, [B, -1, self.window_size])\n",
        "            mask = tf.repeat(mask[:, None, :, :], self.num_heads, axis=1)\n",
        "            attn = tf.where(mask, attn, -1e9)\n",
        "\n",
        "        attn = tf.nn.softmax(attn, axis=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # 聚合Value\n",
        "        out = tf.einsum('bhqk,bkhd->bqhd', attn, v)\n",
        "        out = tf.reshape(out, [B, -1, self.dim])  # (B, T_pad, C)\n",
        "\n",
        "        # 移除填充\n",
        "        if pad_len > 0:\n",
        "            out = out[:, :T, :]\n",
        "\n",
        "        return self.proj(out)"
      ],
      "metadata": {
        "id": "-cKnQ9rqx9NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 新增\n",
        "def SwinTransformerBlock(dim=256, num_heads=4, window_size=16, shift=False, expand=4):\n",
        "    def apply(inputs):\n",
        "        x = inputs\n",
        "\n",
        "        # 窗口注意力\n",
        "        x_norm = tf.keras.layers.LayerNormalization()(x)\n",
        "        attn = WindowedAttention(dim, num_heads, window_size, shift)(x_norm)\n",
        "        x = x + attn  # 残差连接\n",
        "\n",
        "        # 前馈网络\n",
        "        x_norm = tf.keras.layers.LayerNormalization()(x)\n",
        "        ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dim * expand, activation='gelu'),\n",
        "            tf.keras.layers.Dense(dim)\n",
        "        ])(x_norm)\n",
        "        x = x + ffn\n",
        "\n",
        "        return x\n",
        "    return apply"
      ],
      "metadata": {
        "id": "4TXADOf-yEPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIooIcnSqAFR"
      },
      "outputs": [],
      "source": [
        "# 新模型\n",
        "def get_model(max_len=384, dim=192):\n",
        "    inp = tf.keras.Input((max_len, CHANNELS))\n",
        "    x = tf.keras.layers.Masking(mask_value=PAD)(inp)\n",
        "\n",
        "    # Stem层（保留原卷积特征提取）\n",
        "    x = tf.keras.layers.Dense(dim, use_bias=False, name='stem_conv')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(momentum=0.95, name='stem_bn')(x)\n",
        "\n",
        "    # Stage 1: 小窗口细粒度特征\n",
        "    x = Conv1DBlock(dim, 17, drop_rate=0.2)(x)\n",
        "    x = SwinTransformerBlock(dim, window_size=32, shift=False)(x)\n",
        "    x = Conv1DBlock(dim, 17, drop_rate=0.2)(x)\n",
        "    x = SwinTransformerBlock(dim, window_size=32, shift=True)(x)  # 移位窗口\n",
        "\n",
        "    # Stage 2: 中等窗口 + 下采样\n",
        "    x = tf.keras.layers.AvgPool1D(2)(x)  # T=192\n",
        "    x = Conv1DBlock(dim, 17, drop_rate=0.2)(x)\n",
        "    x = SwinTransformerBlock(dim, window_size=24, shift=False)(x)\n",
        "    x = Conv1DBlock(dim, 17, drop_rate=0.2)(x)\n",
        "    x = SwinTransformerBlock(dim, window_size=24, shift=True)(x)\n",
        "\n",
        "    # Stage 3: 大窗口全局上下文\n",
        "    x = tf.keras.layers.AvgPool1D(2)(x)  # T=96\n",
        "    x = Conv1DBlock(dim, 17, drop_rate=0.2)(x)\n",
        "    x = SwinTransformerBlock(dim, window_size=48, shift=False)(x)\n",
        "    x = Conv1DBlock(dim, 17, drop_rate=0.2)(x)\n",
        "    x = SwinTransformerBlock(dim, window_size=48, shift=True)(x)\n",
        "\n",
        "    # 分类头（保持原结构）\n",
        "    x = tf.keras.layers.Dense(dim*2, activation=None, name='top_conv')(x)\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "    x = LateDropout(0.8, start_step=1000)(x)\n",
        "    x = tf.keras.layers.Dense(NUM_CLASSES, name='classifier')(x)\n",
        "\n",
        "    return tf.keras.Model(inp, x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 首先定义一个自定义回调函数，用于根据当前epoch更新数据集\n",
        "class DatasetUpdater(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, train_files, CFG, cutmix_epoch_start=50):\n",
        "        super().__init__()\n",
        "        self.train_files = train_files\n",
        "        self.CFG = CFG\n",
        "        self.cutmix_epoch_start = cutmix_epoch_start\n",
        "        # 保存初始数据集以供第一个epoch使用\n",
        "        self.initial_ds = get_tfrec_dataset(\n",
        "            train_files,\n",
        "            batch_size=CFG.batch_size,\n",
        "            max_len=CFG.max_len,\n",
        "            drop_remainder=True if 'all' != 'all' else False,\n",
        "            augment=True,\n",
        "            repeat=True,\n",
        "            shuffle=32768\n",
        "        )\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        # 注意：epoch从0开始计数\n",
        "        if epoch + 1 >= self.cutmix_epoch_start:  # +1是为了按照人类习惯计数epoch\n",
        "            # 创建带有CutMix的数据集\n",
        "            ds = get_tfrec_dataset(\n",
        "                self.train_files,\n",
        "                batch_size=self.CFG.batch_size,\n",
        "                max_len=self.CFG.max_len,\n",
        "                drop_remainder=True if 'all' != 'all' else False,\n",
        "                augment=True,\n",
        "                repeat=True,\n",
        "                shuffle=32768\n",
        "            )\n",
        "            ds = augment_dataset_with_cutmix(ds, cutmix_prob=0.2, cutmix_ratio=0.2, alpha=0.5)\n",
        "            print(f\"Epoch {epoch+1}: Using CutMix augmentation\")\n",
        "        else:\n",
        "            # 使用没有CutMix的数据集\n",
        "            ds = get_tfrec_dataset(\n",
        "                self.train_files,\n",
        "                batch_size=self.CFG.batch_size,\n",
        "                max_len=self.CFG.max_len,\n",
        "                drop_remainder=True if 'all' != 'all' else False,\n",
        "                augment=True,\n",
        "                repeat=True,\n",
        "                shuffle=32768\n",
        "            )\n",
        "            print(f\"Epoch {epoch+1}: Not using CutMix augmentation\")\n",
        "\n",
        "        # 更新模型的训练数据集\n",
        "        self.model.train_dataset = ds"
      ],
      "metadata": {
        "id": "FgPooav-vsF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoGUEL6-oEWO"
      },
      "outputs": [],
      "source": [
        "def train_fold(CFG, fold, train_files, valid_files=None, strategy=STRATEGY, summary=True, cutmix_epoch_start=30):\n",
        "    seed_everything(CFG.seed)\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "    tf.config.optimizer.set_jit(True)\n",
        "\n",
        "    if CFG.fp16:\n",
        "        try:\n",
        "            policy = mixed_precision.Policy('mixed_bfloat16')\n",
        "            mixed_precision.set_global_policy(policy)\n",
        "        except:\n",
        "            policy = mixed_precision.Policy('mixed_float16')\n",
        "            mixed_precision.set_global_policy(policy)\n",
        "    else:\n",
        "        policy = mixed_precision.Policy('float32')\n",
        "        mixed_precision.set_global_policy(policy)\n",
        "\n",
        "    # 创建初始训练数据集（不带CutMix）\n",
        "    if fold != 'all':\n",
        "        train_ds = get_tfrec_dataset(\n",
        "            train_files,\n",
        "            batch_size=CFG.batch_size,\n",
        "            max_len=CFG.max_len,\n",
        "            drop_remainder=True,\n",
        "            augment=True,\n",
        "            repeat=True,\n",
        "            shuffle=32768\n",
        "        )\n",
        "\n",
        "        valid_ds = get_tfrec_dataset(\n",
        "            valid_files,\n",
        "            batch_size=CFG.batch_size,\n",
        "            max_len=CFG.max_len,\n",
        "            drop_remainder=False,\n",
        "            repeat=False,\n",
        "            shuffle=False\n",
        "        )\n",
        "    else:\n",
        "        train_ds = get_tfrec_dataset(\n",
        "            train_files,\n",
        "            batch_size=CFG.batch_size,\n",
        "            max_len=CFG.max_len,\n",
        "            drop_remainder=False,\n",
        "            augment=True,\n",
        "            repeat=True,\n",
        "            shuffle=32768\n",
        "        )\n",
        "        valid_ds = None\n",
        "        valid_files = []\n",
        "\n",
        "    # 创建数据集更新器回调\n",
        "    dataset_updater = DatasetUpdater(train_files, CFG, cutmix_epoch_start)\n",
        "\n",
        "    num_train = count_data_items(train_files)\n",
        "    num_valid = count_data_items(valid_files) if valid_files else 0\n",
        "    steps_per_epoch = num_train//CFG.batch_size\n",
        "\n",
        "    with strategy.scope():\n",
        "        dropout_step = CFG.dropout_start_epoch * steps_per_epoch\n",
        "        model = get_model(max_len=CFG.max_len, dropout_step=dropout_step, dim=CFG.dim)\n",
        "\n",
        "        schedule = OneCycleLR(CFG.lr, CFG.epoch, warmup_epochs=CFG.epoch*CFG.warmup, steps_per_epoch=steps_per_epoch, resume_epoch=CFG.resume, decay_epochs=CFG.epoch, lr_min=CFG.lr_min, decay_type=CFG.decay_type, warmup_type='linear')\n",
        "        decay_schedule = OneCycleLR(CFG.lr*CFG.weight_decay, CFG.epoch, warmup_epochs=CFG.epoch*CFG.warmup, steps_per_epoch=steps_per_epoch, resume_epoch=CFG.resume, decay_epochs=CFG.epoch, lr_min=CFG.lr_min*CFG.weight_decay, decay_type=CFG.decay_type, warmup_type='linear')\n",
        "\n",
        "        awp_step = CFG.awp_start_epoch * steps_per_epoch\n",
        "        if CFG.fgm:\n",
        "            model = FGM(model.input, model.output, delta=CFG.awp_lambda, eps=0., start_step=awp_step)\n",
        "        elif CFG.awp:\n",
        "            model = AWP(model.input, model.output, delta=CFG.awp_lambda, eps=0., start_step=awp_step)\n",
        "\n",
        "        opt = tfa.optimizers.RectifiedAdam(learning_rate=schedule, weight_decay=decay_schedule, sma_threshold=4, clipvalue=1.)\n",
        "        opt = tfa.optimizers.Lookahead(opt,sync_period=5)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=opt,\n",
        "            loss=[tf.keras.losses.CategoricalCrossentropy(from_logits=True)],\n",
        "            metrics=[\n",
        "                [\n",
        "                tf.keras.metrics.CategoricalAccuracy(),\n",
        "                ],\n",
        "            ],\n",
        "            steps_per_execution=steps_per_epoch,\n",
        "        )\n",
        "\n",
        "    if summary:\n",
        "        print()\n",
        "        model.summary()\n",
        "        print()\n",
        "        print(train_ds, valid_ds)\n",
        "        print()\n",
        "        schedule.plot()\n",
        "        print()\n",
        "        init=False\n",
        "    print(f'---------fold{fold}---------')\n",
        "    print(f'train:{num_train} valid:{num_valid}')\n",
        "    print()\n",
        "\n",
        "    if CFG.resume:\n",
        "        print(f'resume from epoch{CFG.resume}')\n",
        "        model.load_weights(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-last.h5')\n",
        "        if train_ds is not None:\n",
        "            model.evaluate(train_ds.take(steps_per_epoch))\n",
        "        if valid_ds is not None:\n",
        "            model.evaluate(valid_ds)\n",
        "\n",
        "    logger = tf.keras.callbacks.CSVLogger(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-logs.csv')\n",
        "    sv_loss = tf.keras.callbacks.ModelCheckpoint(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-best.h5', monitor='val_loss', verbose=0, save_best_only=True,\n",
        "                save_weights_only=True, mode='min', save_freq='epoch')\n",
        "    snap = Snapshot(f'{CFG.output_dir}/{CFG.comment}-fold{fold}', CFG.snapshot_epochs)\n",
        "    swa = SWA(f'{CFG.output_dir}/{CFG.comment}-fold{fold}', CFG.swa_epochs, strategy=strategy, train_ds=train_ds, valid_ds=valid_ds, valid_steps=-(num_valid//-CFG.batch_size))\n",
        "\n",
        "    # 添加回调\n",
        "    callbacks = [dataset_updater]  # 添加数据集更新器回调\n",
        "    if CFG.save_output:\n",
        "        callbacks.append(logger)\n",
        "        callbacks.append(snap)\n",
        "        callbacks.append(swa)\n",
        "        if fold != 'all':\n",
        "            callbacks.append(sv_loss)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=CFG.epoch-CFG.resume,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        callbacks=callbacks,\n",
        "        validation_data=valid_ds,\n",
        "        verbose=CFG.verbose,\n",
        "        validation_steps=-(num_valid//-CFG.batch_size) if num_valid > 0 else None\n",
        "    )\n",
        "\n",
        "    if CFG.save_output:\n",
        "        try:\n",
        "            model.load_weights(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-best.h5')\n",
        "        except:\n",
        "            pass\n",
        "    if fold != 'all':\n",
        "        cv = model.evaluate(valid_ds,verbose=CFG.verbose,steps=-(num_valid//-CFG.batch_size))\n",
        "    else:\n",
        "        cv = None\n",
        "\n",
        "    return model, cv, history\n",
        "def train_folds(CFG, folds, strategy=STRATEGY, summary=True):\n",
        "    for fold in folds:\n",
        "        if fold != 'all':\n",
        "            all_files = TRAIN_FILENAMES\n",
        "            train_files = [x for x in all_files if f'fold{fold}' not in x]\n",
        "            valid_files = [x for x in all_files if f'fold{fold}' in x]\n",
        "        else:\n",
        "            train_files = TRAIN_FILENAMES\n",
        "            valid_files = None\n",
        "\n",
        "        train_fold(CFG, fold, train_files, valid_files, strategy=strategy, summary=summary)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYUI6mAlqAFR"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    n_splits = 5\n",
        "    save_output = True\n",
        "    output_dir = '.'\n",
        "\n",
        "    seed = 42\n",
        "    verbose = 2 #0) silent 1) progress bar 2) one line per epoch\n",
        "\n",
        "    max_len = 384\n",
        "    replicas = 8\n",
        "    lr = 5e-4 * replicas\n",
        "    weight_decay = 0.1\n",
        "    lr_min = 1e-6\n",
        "    epoch = 400\n",
        "    warmup = 0\n",
        "    batch_size = 64 * replicas\n",
        "    snapshot_epochs = []\n",
        "    swa_epochs = [] #list(range(epoch//2,epoch+1))\n",
        "\n",
        "    fp16 = True\n",
        "    fgm = False\n",
        "    awp = True\n",
        "    awp_lambda = 0.2\n",
        "    awp_start_epoch = 15\n",
        "    dropout_start_epoch = 15\n",
        "    resume = 0\n",
        "    decay_type = 'cosine'\n",
        "    dim = 192\n",
        "    comment = f'islr-fp16-192-8-seed{seed}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8T5GhYPqAFR"
      },
      "outputs": [],
      "source": [
        "# 定义训练和验证文件\n",
        "all_files = TRAIN_FILENAMES\n",
        "fold = 0  # 选择要训练的折\n",
        "train_files = [x for x in all_files if f'fold{fold}' not in x]\n",
        "valid_files = [x for x in all_files if f'fold{fold}' in x]\n",
        "\n",
        "# 然后调用训练函数\n",
        "train_fold(CFG, fold, train_files, valid_files, cutmix_epoch_start=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTJ8eruttPxJ"
      },
      "outputs": [],
      "source": [
        "# CFG.seed = 42\n",
        "# CFG.comment = f'islr-fp16-192-8-seed{CFG.seed}'\n",
        "# train_folds(CFG, ['all'], summary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8IvRT7btP2E"
      },
      "outputs": [],
      "source": [
        "# CFG.seed = 43\n",
        "# CFG.comment = f'islr-fp16-192-8-seed{CFG.seed}'\n",
        "# train_folds(CFG, ['all'], summary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHLmUWM4tP9S"
      },
      "outputs": [],
      "source": [
        "# CFG.seed = 44\n",
        "# CFG.comment = f'islr-fp16-192-8-seed{CFG.seed}'\n",
        "# train_folds(CFG, ['all'], summary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at4Io1dNtSLh"
      },
      "outputs": [],
      "source": [
        "# CFG.seed = 45\n",
        "# CFG.comment = f'islr-fp16-192-8-seed{CFG.seed}'\n",
        "# train_folds(CFG, ['all'], summary=False)"
      ]
    }
  ]
}